<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Receiver (Laptop)</title>
    <script type="module">
        // Import Transformers.js (current official package)
        import {
            AutoProcessor,
            AutoModelForImageTextToText,
            RawImage,
            TextStreamer,
            env
        } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.7.6';

        // Make it available globally
        window.transformers = { AutoProcessor, AutoModelForImageTextToText, RawImage, TextStreamer, env };
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 32px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        
        .main-grid {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .video-container {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 15px;
            padding: 20px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        video {
            width: 100%;
            border-radius: 10px;
            background: black;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            aspect-ratio: 16/9;
        }
        
        .sidebar {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        
        .status-card, .metrics-card, .controls-card {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 20px;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        h3 {
            margin-bottom: 15px;
            font-size: 18px;
            border-bottom: 2px solid rgba(255, 255, 255, 0.3);
            padding-bottom: 10px;
        }
        
        .status-item, .metric-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 12px;
            font-size: 14px;
        }
        
        .status-label, .metric-label {
            opacity: 0.9;
        }
        
        .status-value {
            font-weight: bold;
            padding: 5px 12px;
            border-radius: 20px;
            background: rgba(255, 255, 255, 0.2);
            font-size: 12px;
        }
        
        .status-value.connected {
            background: #10b981;
        }
        
        .status-value.disconnected {
            background: #ef4444;
        }
        
        .status-value.connecting {
            background: #f59e0b;
        }
        
        .metric-value {
            font-weight: bold;
            font-family: monospace;
            background: rgba(0, 0, 0, 0.3);
            padding: 5px 12px;
            border-radius: 8px;
        }
        
        button {
            width: 100%;
            padding: 15px;
            font-size: 16px;
            font-weight: bold;
            border: none;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.3s;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }
        
        button:active {
            transform: scale(0.95);
        }
        
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .btn-primary {
            background: #10b981;
            color: white;
        }
        
        .btn-primary:hover:not(:disabled) {
            background: #059669;
        }
        
        .btn-danger {
            background: #ef4444;
            color: white;
        }
        
        .btn-danger:hover:not(:disabled) {
            background: #dc2626;
        }
        
        .logs {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 15px;
            padding: 20px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        .logs h3 {
            margin-bottom: 15px;
        }
        
        .log-container {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 10px;
            padding: 15px;
            max-height: 300px;
            overflow-y: auto;
            font-family: monospace;
            font-size: 12px;
        }
        
        .log-entry {
            margin-bottom: 5px;
            opacity: 0.9;
        }
        
        .log-entry.error {
            color: #fca5a5;
        }
        
        .log-entry.success {
            color: #86efac;
        }
        
        .log-entry.info {
            color: #bfdbfe;
        }
        
        .waiting-message {
            text-align: center;
            padding: 50px;
            font-size: 18px;
            opacity: 0.7;
        }
        
        @media (max-width: 968px) {
            .main-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸ’» Video Receiver</h1>
        
        <div class="main-grid">
            <div class="video-container">
                <video id="remoteVideo" autoplay playsinline></video>
                <canvas id="frameCanvas" style="display:none;"></canvas>
                <div class="waiting-message" id="waitingMessage">
                    Waiting for video stream from sender...
                </div>
            </div>
            
            <div class="sidebar">
                <div class="status-card">
                    <h3>Connection Status</h3>
                    <div class="status-item">
                        <span class="status-label">Signaling Server:</span>
                        <span class="status-value disconnected" id="signalingStatus">Disconnected</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">Sender:</span>
                        <span class="status-value disconnected" id="senderStatus">Not Connected</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">WebRTC:</span>
                        <span class="status-value disconnected" id="webrtcStatus">Not Connected</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">Video Stream:</span>
                        <span class="status-value disconnected" id="videoStatus">No Stream</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">WebGPU:</span>
                        <span class="status-value disconnected" id="webgpuStatus">Not Initialized</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">AI Model:</span>
                        <span class="status-value disconnected" id="modelStatus">Not Loaded</span>
                    </div>
                    <div class="status-item" id="modelProgressContainer" style="display:none;">
                        <span class="status-label">Download Progress:</span>
                        <span class="status-value" id="modelProgress">0%</span>
                    </div>
                    <div class="status-item" id="modelSpeedContainer" style="display:none;">
                        <span class="status-label">Download Speed:</span>
                        <span class="status-value" id="modelSpeed">- KB/s</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">Connection Type:</span>
                        <span class="status-value disconnected" id="connectionTypeStatus">Unknown</span>
                    </div>
                    <div class="status-item">
                        <span class="status-label">Prompt Mode:</span>
                        <span class="status-value disconnected" id="promptModeStatus">Not Set</span>
                    </div>
                </div>

                <div class="metrics-card">
                    <h3>Stream Metrics</h3>
                    <div class="metric-item">
                        <span class="metric-label">Resolution:</span>
                        <span class="metric-value" id="resolutionMetric">-</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Frame Rate:</span>
                        <span class="metric-value" id="fpsMetric">- fps</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Bitrate:</span>
                        <span class="metric-value" id="bitrateMetric">- kbps</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Packets Lost:</span>
                        <span class="metric-value" id="packetsLostMetric">-</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Latency:</span>
                        <span class="metric-value" id="latencyMetric">- ms</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Last Inference:</span>
                        <span class="metric-value" id="inferenceTimeMetric">- ms</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Tokens Generated:</span>
                        <span class="metric-value" id="tokensMetric">-</span>
                    </div>
                    <div class="metric-item">
                        <span class="metric-label">Tokens/Second:</span>
                        <span class="metric-value" id="tokenRateMetric">- tok/s</span>
                    </div>
                </div>

                <div class="controls-card">
                    <h3>Controls</h3>
                    <label for="promptSelect" style="display: block; margin-bottom: 8px; font-size: 14px;">AI Prompt Mode:</label>
                    <select id="promptSelect" style="width: 100%; padding: 10px; margin-bottom: 15px; border-radius: 8px; border: 1px solid rgba(255,255,255,0.3); background: rgba(0,0,0,0.3); color: white; font-size: 14px;">
                        <option value="concise">One Sentence (Faster, ~1s)</option>
                        <option value="detailed">Detailed Description (Slower, ~2-3s)</option>
                    </select>
                    <button class="btn-primary" id="connectBtn" onclick="connectSignaling()">Connect to Server</button>
                    <button class="btn-danger" id="disconnectBtn" onclick="disconnect()" disabled>Disconnect</button>
                    <button class="btn-primary" id="testBtn" onclick="testDataChannel()" disabled>Test Description</button>
                    <button class="btn-primary" id="startInferenceBtn" onclick="startInference()" disabled>Start AI Analysis</button>
                    <button class="btn-danger" id="stopInferenceBtn" onclick="stopInference()" disabled>Stop AI Analysis</button>
                </div>
            </div>
        </div>
        
        <div class="logs">
            <h3>Activity Log</h3>
            <div class="log-container" id="logs"></div>
        </div>
    </div>

    <script>
        // Configuration
        const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const SIGNALING_SERVER = `${wsProtocol}//${window.location.hostname}:8080`;
        
        // Global variables
        let peerConnection = null;
        let signalingSocket = null;
        let remoteStream = null;
        let statsInterval = null;
        let turnConfig = null;
        let dataChannel = null;

        // WebGPU and AI variables
        let webgpuDevice = null;
        let webgpuAdapter = null;
        let canvas = null;
        let canvasContext = null;
        let imageCapture = null;
        let visionModel = null;
        let visionProcessor = null; // AutoProcessor for FastVLM
        let processingFrame = false;
        let inferenceInterval = null;
        let latestFrame = null;           // Stores most recent captured frame
        let inferenceActive = false;       // Controls processing loop
        
        // Logging
        function log(message, type = 'info') {
            const logsDiv = document.getElementById('logs');
            const entry = document.createElement('div');
            entry.className = `log-entry ${type}`;
            const now = new Date();
            const timestamp = `${now.toLocaleTimeString()}.${now.getMilliseconds().toString().padStart(3, '0')}`;
            entry.textContent = `[${timestamp}] ${message}`;
            logsDiv.appendChild(entry);
            logsDiv.scrollTop = logsDiv.scrollHeight;
            console.log(`[${type.toUpperCase()}] ${message}`);
        }
        
        function updateStatus(element, status, text) {
            const statusEl = document.getElementById(element);
            statusEl.className = `status-value ${status}`;
            statusEl.textContent = text;
        }

        // Initialize WebGPU
        async function initializeWebGPU() {
            try {
                log('Initializing WebGPU...', 'info');
                updateStatus('webgpuStatus', 'connecting', 'Initializing');

                if (!navigator.gpu) {
                    throw new Error('WebGPU not supported in this browser');
                }

                webgpuAdapter = await navigator.gpu.requestAdapter();
                if (!webgpuAdapter) {
                    throw new Error('No WebGPU adapter found');
                }

                webgpuDevice = await webgpuAdapter.requestDevice();

                log('WebGPU initialized successfully', 'success');
                updateStatus('webgpuStatus', 'connected', 'Ready');

                // Initialize canvas for frame extraction
                canvas = document.getElementById('frameCanvas');
                canvasContext = canvas.getContext('2d', { willReadFrequently: true });

                return true;
            } catch (error) {
                log(`WebGPU initialization failed: ${error.message}`, 'error');
                updateStatus('webgpuStatus', 'disconnected', 'Not Available');
                return false;
            }
        }

        // Initialize AI Model (FastVLM-0.5B-ONNX)
        async function initializeModel(retryCount = 0) {
            const MAX_RETRIES = 2;
            const TIMEOUT_MS = 10 * 60 * 1000; // 10 minutes timeout

            try {
                log('Loading FastVLM model (this may take a few minutes)...', 'info');
                updateStatus('modelStatus', 'connecting', 'Loading');

                // Wait for transformers to be available
                while (!window.transformers) {
                    await new Promise(resolve => setTimeout(resolve, 100));
                }

                const { AutoProcessor, AutoModelForImageTextToText, env } = window.transformers;

                // Configure backend preferences
                env.allowLocalModels = false; // Use CDN models
                env.allowRemoteModels = true;

                const model_id = 'onnx-community/FastVLM-0.5B-ONNX';

                // Create timeout promise
                const timeoutPromise = new Promise((_, reject) => {
                    setTimeout(() => reject(new Error('Model loading timeout (10 minutes exceeded)')), TIMEOUT_MS);
                });

                // Track download progress and speed
                let lastProgressTime = Date.now();
                let lastProgressLoaded = 0;
                const downloadStartTime = Date.now();

                // Show progress indicators
                document.getElementById('modelProgressContainer').style.display = 'flex';
                document.getElementById('modelSpeedContainer').style.display = 'flex';

                // Enhanced progress callback with speed tracking
                const createProgressCallback = (fileType) => (progress) => {
                    if (progress.status === 'downloading') {
                        const percent = Math.round((progress.loaded / progress.total) * 100);
                        const loadedMB = Math.round(progress.loaded / 1024 / 1024);
                        const totalMB = Math.round(progress.total / 1024 / 1024);

                        // Calculate download speed
                        const now = Date.now();
                        const timeDiff = (now - lastProgressTime) / 1000; // seconds
                        const bytesDiff = progress.loaded - lastProgressLoaded;

                        if (timeDiff > 0.5) { // Update speed every 0.5 seconds
                            const speedKBps = Math.round((bytesDiff / 1024) / timeDiff);
                            const speedMBps = (speedKBps / 1024).toFixed(2);

                            // Estimate time remaining
                            const bytesRemaining = progress.total - progress.loaded;
                            const secondsRemaining = bytesRemaining / (bytesDiff / timeDiff);
                            const minutesRemaining = Math.ceil(secondsRemaining / 60);

                            // Update UI
                            document.getElementById('modelProgress').textContent = `${percent}% (${loadedMB}/${totalMB} MB)`;
                            document.getElementById('modelSpeed').textContent =
                                speedKBps > 1024 ? `${speedMBps} MB/s` : `${speedKBps} KB/s`;

                            // Log with ETA
                            if (fileType === 'model') {
                                log(`Downloading ${progress.file}: ${percent}% (${loadedMB}MB / ${totalMB}MB) - ${speedKBps} KB/s - ETA: ${minutesRemaining} min`, 'info');
                            }

                            updateStatus('modelStatus', 'connecting', `${percent}%`);

                            lastProgressTime = now;
                            lastProgressLoaded = progress.loaded;
                        }
                    } else if (progress.status === 'ready') {
                        log(`${progress.file} ready`, 'success');
                    } else if (progress.status === 'done') {
                        const totalTime = ((Date.now() - downloadStartTime) / 1000).toFixed(1);
                        log(`${fileType} download completed in ${totalTime} seconds`, 'success');
                    }
                };

                // Load processor with timeout
                log('Loading FastVLM processor...', 'info');
                const processorPromise = AutoProcessor.from_pretrained(model_id, {
                    progress_callback: createProgressCallback('processor')
                });
                visionProcessor = await Promise.race([processorPromise, timeoutPromise]);
                log('Processor loaded successfully', 'success');

                // Reset progress tracking for model
                lastProgressTime = Date.now();
                lastProgressLoaded = 0;

                // Load model with quantization config and timeout
                log('Loading FastVLM model (120-160MB, quantized)...', 'info');
                log('IMPORTANT: This is downloading from CDN, may take 3-10 minutes depending on connection', 'info');
                const modelPromise = AutoModelForImageTextToText.from_pretrained(model_id, {
                    dtype: {
                        embed_tokens: 'fp16',           // Moderate precision for text embeddings
                        vision_encoder: 'q4',           // Aggressive 4-bit for vision
                        decoder_model_merged: 'q4',     // 4-bit for language model
                    },
                    device: 'webgpu',                   // Prefer WebGPU
                    progress_callback: createProgressCallback('model')
                });
                visionModel = await Promise.race([modelPromise, timeoutPromise]);

                // Hide progress indicators when done
                document.getElementById('modelProgressContainer').style.display = 'none';
                document.getElementById('modelSpeedContainer').style.display = 'none';

                log('FastVLM model loaded successfully', 'success');
                log('Model quantization: fp16 (embeddings) + q4 (vision + decoder)', 'info');

                // Verify WebGPU backend is actually being used
                try {
                    // Try to access model device/backend information
                    if (visionModel.config && visionModel.config.model_type) {
                        log(`Model type: ${visionModel.config.model_type}`, 'info');
                    }

                    // Check if WebGPU adapter was initialized
                    if (webgpuAdapter) {
                        // Use adapter.limits and adapter.features (universally supported)
                        const limits = webgpuAdapter.limits;
                        const features = Array.from(webgpuAdapter.features);

                        log(`WebGPU Adapter detected`, 'success');
                        log(`Max buffer size: ${(limits.maxBufferSize / 1024 / 1024).toFixed(0)} MB`, 'info');
                        log(`Max compute workgroup size: ${limits.maxComputeWorkgroupSizeX}`, 'info');
                        log(`WebGPU features: ${features.slice(0, 3).join(', ')}${features.length > 3 ? '...' : ''}`, 'info');

                        // Try to get vendor info from device if available
                        if (webgpuDevice) {
                            try {
                                const deviceInfo = webgpuDevice.adapterInfo || {};
                                if (deviceInfo.vendor) {
                                    log(`GPU Vendor: ${deviceInfo.vendor}`, 'success');
                                    updateStatus('webgpuStatus', 'connected', `Ready (${deviceInfo.vendor})`);
                                } else {
                                    log('Confirmed: Model using WebGPU backend for inference', 'success');
                                    updateStatus('webgpuStatus', 'connected', 'Ready (WebGPU)');
                                }
                            } catch (e) {
                                log('Confirmed: Model using WebGPU backend for inference', 'success');
                                updateStatus('webgpuStatus', 'connected', 'Ready (WebGPU)');
                            }
                        } else {
                            log('Confirmed: WebGPU adapter initialized', 'success');
                            updateStatus('webgpuStatus', 'connected', 'Ready');
                        }
                    } else {
                        log('WARNING: WebGPU adapter not found - may be using CPU fallback', 'error');
                        updateStatus('webgpuStatus', 'warning', 'CPU Fallback?');
                    }
                } catch (e) {
                    log(`Could not verify WebGPU backend: ${e.message}`, 'error');
                }

                updateStatus('modelStatus', 'connected', 'Ready');

                // Enable inference button
                document.getElementById('startInferenceBtn').disabled = false;

                return true;
            } catch (error) {
                log(`Model loading failed: ${error.message}`, 'error');
                log('Error stack: ' + error.stack, 'error');

                // Check for specific errors and retry logic
                if (error.message.includes('timeout') && retryCount < MAX_RETRIES) {
                    const waitTime = Math.pow(2, retryCount) * 5000; // Exponential backoff: 5s, 10s, 20s
                    log(`Retrying model load in ${waitTime / 1000} seconds (attempt ${retryCount + 1}/${MAX_RETRIES})...`, 'info');
                    updateStatus('modelStatus', 'connecting', `Retry ${retryCount + 1}/${MAX_RETRIES}`);

                    await new Promise(resolve => setTimeout(resolve, waitTime));
                    return initializeModel(retryCount + 1);
                } else if (error.message.includes('WebGPU')) {
                    log('WebGPU not available, falling back to WASM...', 'info');
                    updateStatus('modelStatus', 'disconnected', 'WebGPU Failed');
                    // Could retry with device: 'wasm' here in future enhancement
                } else if (error.message.includes('NetworkError') || error.message.includes('fetch')) {
                    log('Network error during model download. Check internet connection and CDN accessibility.', 'error');
                    log('Try: 1) Check network connection, 2) Clear browser cache, 3) Retry', 'info');
                    updateStatus('modelStatus', 'disconnected', 'Network Error');
                } else {
                    updateStatus('modelStatus', 'disconnected', 'Failed');
                }

                // Final failure - provide helpful instructions
                if (retryCount >= MAX_RETRIES) {
                    log('Model loading failed after all retries. Troubleshooting steps:', 'error');
                    log('1. Check internet connection and CDN accessibility', 'info');
                    log('2. Clear browser cache (DevTools â†’ Application â†’ Storage â†’ Clear)', 'info');
                    log('3. Try using Chrome instead of Safari', 'info');
                    log('4. Run diagnostic: ./test-network-connectivity.sh', 'info');
                }

                return false;
            }
        }

        // Capture frame from video and create RawImage
        function captureFrame() {
            const video = document.getElementById('remoteVideo');

            if (!video || video.readyState !== video.HAVE_ENOUGH_DATA) {
                return null;
            }

            // Set canvas size to match video
            const width = video.videoWidth || 640;
            const height = video.videoHeight || 480;
            canvas.width = width;
            canvas.height = height;

            // Draw current video frame to canvas
            canvasContext.drawImage(video, 0, 0, width, height);

            // Get ImageData from canvas
            const imageData = canvasContext.getImageData(0, 0, width, height);

            // Create RawImage for FastVLM (more efficient than toDataURL)
            const { RawImage } = window.transformers;
            const rawImage = new RawImage(imageData.data, width, height, 4); // 4 channels (RGBA)

            return rawImage;
        }

        // Process frame with FastVLM
        async function processFrame() {
            if (processingFrame || !visionModel || !visionProcessor) {
                return;
            }

            processingFrame = true;

            try {
                const frame = captureFrame();
                if (!frame) {
                    processingFrame = false;
                    return;
                }

                log('Processing frame with FastVLM...', 'info');

                // Prepare chat messages for FastVLM
                const messages = [
                    {
                        role: 'user',
                        content: '<image>Describe this image in detail for a visually impaired person. Include objects, people, actions, colors, and spatial relationships.'
                    }
                ];

                // Apply chat template
                const prompt = visionProcessor.apply_chat_template(messages, {
                    add_generation_prompt: true
                });

                // Process image + prompt together
                const inputs = await visionProcessor(frame, prompt, {
                    add_special_tokens: false
                });

                log('Running FastVLM inference...', 'info');

                // Generate description
                const outputs = await visionModel.generate({
                    ...inputs,
                    max_new_tokens: 512,
                    do_sample: false,           // Deterministic output
                    repetition_penalty: 1.2,    // Avoid repetitive text
                });

                // Decode the output
                const decoded = visionProcessor.batch_decode(
                    outputs.slice(null, [inputs.input_ids.dims.at(-1), null]),
                    { skip_special_tokens: true }
                );

                const description = decoded[0];

                if (description) {
                    log(`Generated description: ${description}`, 'success');

                    // Send description through data channel
                    sendDescription(description);
                } else {
                    log('No description generated', 'error');
                }

            } catch (error) {
                log(`Frame processing error: ${error.message}`, 'error');
                log('Error stack: ' + error.stack, 'error');
            } finally {
                processingFrame = false;
            }
        }

        // Run inference on a given frame (separated from capture)
        async function runInference(frame) {
            try {
                log('Processing frame with FastVLM...', 'info');
                const inferenceStartTime = performance.now();

                // Get selected prompt mode
                const promptMode = document.getElementById('promptSelect').value;
                const promptText = promptMode === 'concise'
                    ? '<image>Describe what you see in one sentence.'
                    : '<image>Describe this image in detail for a visually impaired person. Include objects, people, actions, colors, and spatial relationships.';

                log(`Using prompt mode: ${promptMode}`, 'info');
                log(`Prompt text: ${promptText}`, 'info');

                // Update prompt mode status indicator
                updateStatus('promptModeStatus', 'connected',
                    promptMode === 'concise' ? 'Fast (1 sentence)' : 'Detailed (full desc)');

                const messages = [{
                    role: 'user',
                    content: promptText
                }];

                // Timing: Chat template
                const templateStartTime = performance.now();
                const prompt = visionProcessor.apply_chat_template(messages, {
                    add_generation_prompt: true
                });
                const templateTime = (performance.now() - templateStartTime).toFixed(0);
                log(`[Timing] Chat template: ${templateTime}ms`, 'info');

                // Timing: Image preprocessing
                const preprocessStartTime = performance.now();
                const inputs = await visionProcessor(frame, prompt, {
                    add_special_tokens: false
                });
                const preprocessTime = (performance.now() - preprocessStartTime).toFixed(0);
                log(`[Timing] Preprocessing: ${preprocessTime}ms`, 'info');

                log('Running FastVLM inference...', 'info');

                // Adjust max tokens based on prompt mode
                const maxTokens = promptMode === 'concise' ? 30 : 200;
                log(`Max tokens allowed: ${maxTokens}`, 'info');

                // Timing: Token generation
                const generationStartTime = performance.now();
                const outputs = await visionModel.generate({
                    ...inputs,
                    max_new_tokens: maxTokens,  // 30 for concise, 200 for detailed
                    do_sample: false,
                    repetition_penalty: 1.2,
                });
                const generationTime = (performance.now() - generationStartTime).toFixed(0);

                // Calculate tokens generated
                const inputTokens = inputs.input_ids.dims.at(-1);
                const outputTokens = outputs.dims.at(-1);
                const tokensGenerated = outputTokens - inputTokens;
                const tokenRate = (tokensGenerated / (generationTime / 1000)).toFixed(1);

                log(`[Timing] Token generation: ${generationTime}ms (${tokensGenerated} tokens)`, 'info');
                log(`[Timing] Token rate: ${tokenRate} tok/s`, 'info');

                // Timing: Decoding
                const decodeStartTime = performance.now();
                const decoded = visionProcessor.batch_decode(
                    outputs.slice(null, [inputs.input_ids.dims.at(-1), null]),
                    { skip_special_tokens: true }
                );
                const decodeTime = (performance.now() - decodeStartTime).toFixed(0);
                log(`[Timing] Decoding: ${decodeTime}ms`, 'info');

                const description = decoded[0];
                const inferenceTime = (performance.now() - inferenceStartTime).toFixed(0);

                log(`[Timing] Total inference: ${inferenceTime}ms`, 'success');

                // Update performance metrics in UI
                if (document.getElementById('inferenceTimeMetric')) {
                    document.getElementById('inferenceTimeMetric').textContent = `${inferenceTime} ms`;
                }
                if (document.getElementById('tokensMetric')) {
                    document.getElementById('tokensMetric').textContent = tokensGenerated;
                }
                if (document.getElementById('tokenRateMetric')) {
                    document.getElementById('tokenRateMetric').textContent = `${tokenRate} tok/s`;
                }

                // Performance warning if inference is too slow
                if (inferenceTime > 3000) {
                    log(`WARNING: Inference took ${inferenceTime}ms (expected <2000ms for concise mode). Check GPU usage in Activity Monitor.`, 'error');
                }

                // WebGPU performance confirmation (log once on first inference after 2nd run when warmed up)
                if (tokenRate > 15) {
                    log(`WebGPU Status: CONFIRMED - Inference running at ${tokenRate} tok/s`, 'success');
                    log('GPU acceleration confirmed (high token rate)', 'success');
                } else if (tokenRate < 10) {
                    log(`WARNING: Low token rate (${tokenRate} tok/s) may indicate CPU fallback`, 'error');
                }

                if (description) {
                    log(`Generated description (${inferenceTime}ms): ${description}`, 'success');
                    sendDescription(description);
                } else {
                    log('No description generated', 'error');
                }

            } catch (error) {
                log(`Inference error: ${error.message}`, 'error');
            }
        }

        // Continuous processing loop - processes latest available frame
        async function startProcessingLoop() {
            inferenceActive = true;

            while (inferenceActive) {
                if (!processingFrame && latestFrame) {
                    processingFrame = true;

                    // Grab the latest frame and clear it
                    const frameToProcess = latestFrame;
                    latestFrame = null;

                    // Run inference
                    await runInference(frameToProcess);

                    processingFrame = false;
                }

                // Check for new frames every 100ms
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        }

        // Start continuous inference
        function startInference() {
            if (inferenceInterval) {
                clearInterval(inferenceInterval);
            }

            // Process first frame immediately (don't wait for interval)
            const firstFrame = captureFrame();
            if (firstFrame) {
                latestFrame = firstFrame;
            }

            // Start continuous processing loop
            startProcessingLoop();

            // Capture frames every 3 seconds
            inferenceInterval = setInterval(() => {
                const frame = captureFrame();
                if (frame) {
                    latestFrame = frame;  // Overwrites old frame
                    log('Captured new frame for processing', 'info');
                }
            }, 3000);

            document.getElementById('startInferenceBtn').disabled = true;
            document.getElementById('stopInferenceBtn').disabled = false;

            log('Started continuous inference (capturing every 3 seconds, processing latest)', 'success');
        }

        // Stop continuous inference
        function stopInference() {
            inferenceActive = false;  // Stops processing loop

            if (inferenceInterval) {
                clearInterval(inferenceInterval);
                inferenceInterval = null;
            }

            latestFrame = null;  // Clear any pending frame
            processingFrame = false;

            document.getElementById('startInferenceBtn').disabled = false;
            document.getElementById('stopInferenceBtn').disabled = true;

            log('Stopped continuous inference', 'info');
        }

        // Connect to signaling server
        function connectSignaling() {
            log('Connecting to signaling server...', 'info');
            updateStatus('signalingStatus', 'connecting', 'Connecting');
            
            signalingSocket = new WebSocket(SIGNALING_SERVER);
            
            signalingSocket.onopen = () => {
                log('Connected to signaling server', 'success');
                updateStatus('signalingStatus', 'connected', 'Connected');
                
                // Register as receiver
                signalingSocket.send(JSON.stringify({
                    type: 'register',
                    role: 'receiver'
                }));
                
                document.getElementById('connectBtn').disabled = true;
                document.getElementById('disconnectBtn').disabled = false;
            };
            
            signalingSocket.onmessage = async (event) => {
                const message = JSON.parse(event.data);
                log(`Received: ${message.type}`, 'info');
                
                switch(message.type) {
                    case 'welcome':
                        turnConfig = message.turnConfig;
                        log('Received TURN configuration', 'success');
                        break;
                        
                    case 'registered':
                        log('Registered as receiver', 'success');
                        turnConfig = message.turnConfig;
                        break;
                        
                    case 'peer-connected':
                        if (message.peer === 'sender') {
                            log('Sender connected! Waiting for offer...', 'success');
                            updateStatus('senderStatus', 'connected', 'Connected');
                        }
                        break;
                        
                    case 'offer':
                        log('Received offer from sender', 'info');
                        await handleOffer(message.sdp);
                        break;
                        
                    case 'ice-candidate':
                        if (message.candidate && peerConnection) {
                            log('Received ICE candidate from sender', 'info');
                            await peerConnection.addIceCandidate(new RTCIceCandidate(message.candidate));
                        }
                        break;
                        
                    case 'peer-disconnected':
                        if (message.peer === 'sender') {
                            log('Sender disconnected', 'error');
                            updateStatus('senderStatus', 'disconnected', 'Disconnected');
                            updateStatus('webrtcStatus', 'disconnected', 'Disconnected');
                            updateStatus('videoStatus', 'disconnected', 'No Stream');
                            document.getElementById('waitingMessage').style.display = 'block';
                            if (statsInterval) {
                                clearInterval(statsInterval);
                            }
                        }
                        break;
                        
                    case 'error':
                        log(`Server error: ${message.message}`, 'error');
                        break;
                }
            };
            
            signalingSocket.onerror = (error) => {
                log('WebSocket error', 'error');
                updateStatus('signalingStatus', 'disconnected', 'Error');
            };
            
            signalingSocket.onclose = () => {
                log('Signaling connection closed', 'error');
                updateStatus('signalingStatus', 'disconnected', 'Disconnected');
                document.getElementById('connectBtn').disabled = false;
                document.getElementById('disconnectBtn').disabled = true;
            };
        }

        // Send description through data channel
        function sendDescription(text) {
            if (dataChannel && dataChannel.readyState === 'open') {
                const message = JSON.stringify({
                    type: 'description',
                    text: text,
                    timestamp: Date.now()
                });
                dataChannel.send(message);
                log(`Sent description: ${text.substring(0, 50)}...`, 'success');
                return true;
            } else {
                log('Data channel not ready', 'error');
                return false;
            }
        }

        // Test data channel with a sample description
        function testDataChannel() {
            const testDescriptions = [
                'A person is standing in a bright room with natural lighting from a window.',
                'I can see a computer screen displaying code in a dark theme editor.',
                'There appears to be a wooden desk with various objects on it.',
                'The scene shows an indoor environment with furniture visible in the background.',
                'A mobile device is being held up, showing the camera view.'
            ];

            const randomDesc = testDescriptions[Math.floor(Math.random() * testDescriptions.length)];
            sendDescription(randomDesc);
        }

        // Handle incoming offer from sender
        async function handleOffer(sdp) {
            try {
                log('Creating peer connection...', 'info');
                updateStatus('webrtcStatus', 'connecting', 'Connecting');
                
                // Create peer connection with TURN server
                peerConnection = new RTCPeerConnection(turnConfig);

                // Handle data channel from sender
                peerConnection.ondatachannel = (event) => {
                    dataChannel = event.channel;
                    log('Data channel received', 'success');

                    dataChannel.onopen = () => {
                        log('Data channel opened - ready to send descriptions', 'success');
                        document.getElementById('testBtn').disabled = false;
                    };

                    dataChannel.onclose = () => {
                        log('Data channel closed', 'info');
                        document.getElementById('testBtn').disabled = true;
                    };

                    dataChannel.onerror = (error) => {
                        log('Data channel error: ' + error, 'error');
                    };

                    dataChannel.onmessage = (event) => {
                        log('Received message from sender: ' + event.data, 'info');
                    };
                };

                // Handle incoming tracks
                peerConnection.ontrack = async (event) => {
                    log('Received remote track', 'success');
                    remoteStream = event.streams[0];
                    document.getElementById('remoteVideo').srcObject = remoteStream;
                    document.getElementById('waitingMessage').style.display = 'none';
                    updateStatus('videoStatus', 'connected', 'Receiving');

                    // Start collecting metrics
                    startMetricsCollection();

                    // Initialize WebGPU and AI model
                    const webgpuReady = await initializeWebGPU();
                    if (webgpuReady) {
                        const modelReady = await initializeModel();
                        if (modelReady) {
                            // Start automatic inference when video is playing
                            const video = document.getElementById('remoteVideo');
                            video.addEventListener('playing', () => {
                                setTimeout(() => {
                                    startInference();
                                }, 2000); // Wait 2 seconds for video to stabilize
                            }, { once: true });
                        }
                    }
                };
                
                // Handle ICE candidates
                peerConnection.onicecandidate = (event) => {
                    if (event.candidate) {
                        const connType = analyzeConnectionType(event.candidate);
                        log(`ICE candidate (${connType}): ${event.candidate.candidate.substring(0, 50)}...`, 'info');
                        signalingSocket.send(JSON.stringify({
                            type: 'ice-candidate',
                            candidate: event.candidate
                        }));
                    }
                };
                
                // Handle connection state changes
                peerConnection.onconnectionstatechange = () => {
                    log(`Connection state: ${peerConnection.connectionState}`, 'info');
                    
                    switch(peerConnection.connectionState) {
                        case 'connected':
                            updateStatus('webrtcStatus', 'connected', 'Connected');
                            log('WebRTC connection established!', 'success');
                            break;
                        case 'disconnected':
                        case 'failed':
                            updateStatus('webrtcStatus', 'disconnected', 'Failed');
                            updateStatus('videoStatus', 'disconnected', 'No Stream');
                            log('Connection failed or disconnected', 'error');
                            if (statsInterval) {
                                clearInterval(statsInterval);
                            }
                            break;
                        case 'closed':
                            updateStatus('webrtcStatus', 'disconnected', 'Closed');
                            updateStatus('videoStatus', 'disconnected', 'No Stream');
                            break;
                    }
                };
                
                // Handle ICE connection state changes
                peerConnection.oniceconnectionstatechange = () => {
                    log(`ICE connection state: ${peerConnection.iceConnectionState}`, 'info');
                };
                
                // Set remote description (offer)
                await peerConnection.setRemoteDescription(new RTCSessionDescription({
                    type: 'offer',
                    sdp: sdp
                }));
                
                // Create and send answer
                const answer = await peerConnection.createAnswer();
                await peerConnection.setLocalDescription(answer);
                
                log('Sending answer to sender...', 'info');
                signalingSocket.send(JSON.stringify({
                    type: 'answer',
                    sdp: answer.sdp
                }));
                
            } catch (error) {
                log(`Error handling offer: ${error.message}`, 'error');
                updateStatus('webrtcStatus', 'disconnected', 'Error');
            }
        }

        // Analyze ICE candidate to determine connection type
        function analyzeConnectionType(candidate) {
            if (!candidate || !candidate.candidate) return 'unknown';

            const candidateStr = candidate.candidate;

            // Check candidate type in the SDP string
            if (candidateStr.includes('typ host')) {
                return 'P2P (Local)';
            } else if (candidateStr.includes('typ srflx')) {
                return 'P2P (STUN)';
            } else if (candidateStr.includes('typ relay')) {
                return 'TURN Relay';
            } else if (candidateStr.includes('typ prflx')) {
                return 'P2P (Peer Reflexive)';
            }

            return 'Unknown';
        }

        // Start collecting and displaying metrics
        function startMetricsCollection() {
            if (statsInterval) {
                clearInterval(statsInterval);
            }
            
            statsInterval = setInterval(async () => {
                if (!peerConnection) return;
                
                try {
                    const stats = await peerConnection.getStats();
                    let inboundRtpStats = null;

                    stats.forEach(report => {
                        if (report.type === 'inbound-rtp' && report.kind === 'video') {
                            inboundRtpStats = report;
                        }

                        // Check connection type via ICE candidate pairs
                        if (report.type === 'candidate-pair' && report.state === 'succeeded') {
                            const localCandidateId = report.localCandidateId;

                            // Find the actual local candidate
                            stats.forEach(candidate => {
                                if (candidate.id === localCandidateId) {
                                    const connType = analyzeConnectionType(candidate);
                                    updateStatus('connectionTypeStatus',
                                        connType.includes('P2P') ? 'connected' : 'warning',
                                        connType);
                                }
                            });
                        }
                    });

                    if (inboundRtpStats) {
                        // Resolution
                        if (inboundRtpStats.frameWidth && inboundRtpStats.frameHeight) {
                            document.getElementById('resolutionMetric').textContent = 
                                `${inboundRtpStats.frameWidth}x${inboundRtpStats.frameHeight}`;
                        }
                        
                        // Frame rate
                        if (inboundRtpStats.framesPerSecond) {
                            document.getElementById('fpsMetric').textContent = 
                                `${Math.round(inboundRtpStats.framesPerSecond)} fps`;
                        }
                        
                        // Bitrate (calculate from bytes received)
                        if (inboundRtpStats.bytesReceived !== undefined) {
                            const bitrate = Math.round((inboundRtpStats.bytesReceived * 8) / 1000);
                            document.getElementById('bitrateMetric').textContent = 
                                `${bitrate} kbps`;
                        }
                        
                        // Packets lost
                        if (inboundRtpStats.packetsLost !== undefined) {
                            document.getElementById('packetsLostMetric').textContent = 
                                inboundRtpStats.packetsLost;
                        }
                        
                        // Jitter (as proxy for latency)
                        if (inboundRtpStats.jitter !== undefined) {
                            document.getElementById('latencyMetric').textContent = 
                                `${Math.round(inboundRtpStats.jitter * 1000)} ms`;
                        }
                    }
                } catch (error) {
                    console.error('Error collecting stats:', error);
                }
            }, 1000);
        }
        
        // Disconnect
        function disconnect() {
            log('Disconnecting...', 'info');

            // Stop inference
            stopInference();

            if (dataChannel) {
                dataChannel.close();
                dataChannel = null;
            }

            if (statsInterval) {
                clearInterval(statsInterval);
                statsInterval = null;
            }

            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }

            if (signalingSocket) {
                signalingSocket.close();
                signalingSocket = null;
            }
            
            document.getElementById('remoteVideo').srcObject = null;
            document.getElementById('waitingMessage').style.display = 'block';
            
            updateStatus('signalingStatus', 'disconnected', 'Disconnected');
            updateStatus('senderStatus', 'disconnected', 'Not Connected');
            updateStatus('webrtcStatus', 'disconnected', 'Not Connected');
            updateStatus('videoStatus', 'disconnected', 'No Stream');
            
            // Reset metrics
            document.getElementById('resolutionMetric').textContent = '-';
            document.getElementById('fpsMetric').textContent = '- fps';
            document.getElementById('bitrateMetric').textContent = '- kbps';
            document.getElementById('packetsLostMetric').textContent = '-';
            document.getElementById('latencyMetric').textContent = '- ms';
            
            document.getElementById('connectBtn').disabled = false;
            document.getElementById('disconnectBtn').disabled = true;
            
            log('Disconnected', 'success');
        }
        
        // Initialize
        log('Receiver page loaded', 'success');
        log(`Signaling server: ${SIGNALING_SERVER}`, 'info');
    </script>
</body>
</html>
